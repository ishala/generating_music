{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Pelajaran\\Semester 7\\Riset\\Projek Coba-Coba\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>judul_lagu</th>\n",
       "      <th>bar_number</th>\n",
       "      <th>bar</th>\n",
       "      <th>pitch_pattern</th>\n",
       "      <th>birama</th>\n",
       "      <th>panjang_note</th>\n",
       "      <th>ritme</th>\n",
       "      <th>kunci</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Enchanted Valley</td>\n",
       "      <td>1</td>\n",
       "      <td>G3-A (Bcd=e)</td>\n",
       "      <td>flat</td>\n",
       "      <td>2/4</td>\n",
       "      <td>1/16</td>\n",
       "      <td>Very slow</td>\n",
       "      <td>Gm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The Enchanted Valley</td>\n",
       "      <td>2</td>\n",
       "      <td>f4 (g2dB)</td>\n",
       "      <td>flat</td>\n",
       "      <td>2/4</td>\n",
       "      <td>1/16</td>\n",
       "      <td>Very slow</td>\n",
       "      <td>Gm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>The Enchanted Valley</td>\n",
       "      <td>3</td>\n",
       "      <td>({d}c3-B) G2-E2</td>\n",
       "      <td>flat</td>\n",
       "      <td>2/4</td>\n",
       "      <td>1/16</td>\n",
       "      <td>Very slow</td>\n",
       "      <td>Gm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>The Enchanted Valley</td>\n",
       "      <td>4</td>\n",
       "      <td>F4 (D2=E^F)</td>\n",
       "      <td>flat</td>\n",
       "      <td>2/4</td>\n",
       "      <td>1/16</td>\n",
       "      <td>Very slow</td>\n",
       "      <td>Gm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The Enchanted Valley</td>\n",
       "      <td>5</td>\n",
       "      <td>G3-A (Bcd=e)</td>\n",
       "      <td>flat</td>\n",
       "      <td>2/4</td>\n",
       "      <td>1/16</td>\n",
       "      <td>Very slow</td>\n",
       "      <td>Gm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>The Enchanted Valley</td>\n",
       "      <td>6</td>\n",
       "      <td>f4 d2-f2</td>\n",
       "      <td>flat</td>\n",
       "      <td>2/4</td>\n",
       "      <td>1/16</td>\n",
       "      <td>Very slow</td>\n",
       "      <td>Gm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>The Enchanted Valley</td>\n",
       "      <td>7</td>\n",
       "      <td>(g2a2 b2).g2</td>\n",
       "      <td>flat</td>\n",
       "      <td>2/4</td>\n",
       "      <td>1/16</td>\n",
       "      <td>Very slow</td>\n",
       "      <td>Gm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>The Enchanted Valley</td>\n",
       "      <td>8</td>\n",
       "      <td>{b}(a2g2 f2).d2</td>\n",
       "      <td>down</td>\n",
       "      <td>2/4</td>\n",
       "      <td>1/16</td>\n",
       "      <td>Very slow</td>\n",
       "      <td>Gm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>The Enchanted Valley</td>\n",
       "      <td>9</td>\n",
       "      <td>(d2{ed}c2) B2B2</td>\n",
       "      <td>flat</td>\n",
       "      <td>2/4</td>\n",
       "      <td>1/16</td>\n",
       "      <td>Very slow</td>\n",
       "      <td>Gm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>The Enchanted Valley</td>\n",
       "      <td>10</td>\n",
       "      <td>(A2G2 {AG}F2).D2</td>\n",
       "      <td>flat</td>\n",
       "      <td>2/4</td>\n",
       "      <td>1/16</td>\n",
       "      <td>Very slow</td>\n",
       "      <td>Gm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index            judul_lagu  bar_number               bar pitch_pattern  \\\n",
       "0      1  The Enchanted Valley           1      G3-A (Bcd=e)          flat   \n",
       "1      1  The Enchanted Valley           2         f4 (g2dB)          flat   \n",
       "2      1  The Enchanted Valley           3   ({d}c3-B) G2-E2          flat   \n",
       "3      1  The Enchanted Valley           4       F4 (D2=E^F)          flat   \n",
       "4      1  The Enchanted Valley           5      G3-A (Bcd=e)          flat   \n",
       "5      1  The Enchanted Valley           6          f4 d2-f2          flat   \n",
       "6      1  The Enchanted Valley           7      (g2a2 b2).g2          flat   \n",
       "7      1  The Enchanted Valley           8   {b}(a2g2 f2).d2          down   \n",
       "8      1  The Enchanted Valley           9   (d2{ed}c2) B2B2          flat   \n",
       "9      1  The Enchanted Valley          10  (A2G2 {AG}F2).D2          flat   \n",
       "\n",
       "  birama panjang_note      ritme kunci  \n",
       "0    2/4         1/16  Very slow    Gm  \n",
       "1    2/4         1/16  Very slow    Gm  \n",
       "2    2/4         1/16  Very slow    Gm  \n",
       "3    2/4         1/16  Very slow    Gm  \n",
       "4    2/4         1/16  Very slow    Gm  \n",
       "5    2/4         1/16  Very slow    Gm  \n",
       "6    2/4         1/16  Very slow    Gm  \n",
       "7    2/4         1/16  Very slow    Gm  \n",
       "8    2/4         1/16  Very slow    Gm  \n",
       "9    2/4         1/16  Very slow    Gm  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = 'data/new/cleaned.csv'\n",
    "df = pd.read_csv(data)\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Fitur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers By GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import random\n",
    "\n",
    "# Fungsi untuk memproses kolom 'bar'\n",
    "def process_abc(abc_notation, tokenizer):\n",
    "    tokens = tokenize_abc(abc_notation)\n",
    "    processed = [tokenizer.get(token, tokenizer['[UNK]']) for token in tokens]\n",
    "    original_length = len(processed)\n",
    "    return processed, original_length\n",
    "\n",
    "# Fungsi untuk membuat tokenizer berdasarkan kolom 'bar'\n",
    "def create_tokenizer(df):\n",
    "    tokenizer = {'[PAD]': 0, '[UNK]': 1}  # Menambahkan token PAD dan UNK\n",
    "    token_id = 2\n",
    "    for notation in df['bar']:\n",
    "        if pd.notnull(notation):\n",
    "            tokens = tokenize_abc(notation)\n",
    "            for token in tokens:\n",
    "                if token not in tokenizer:\n",
    "                    tokenizer[token] = token_id\n",
    "                    token_id += 1\n",
    "    return tokenizer\n",
    "\n",
    "# Fungsi untuk mengubah notasi ABC menjadi token\n",
    "def tokenize_abc(abc_notation):\n",
    "    return list(abc_notation)\n",
    "\n",
    "# Label encoder untuk kolom lainnya\n",
    "encoder = LabelEncoder()\n",
    "pickedCol = ['bar', 'pitch_pattern', 'birama', 'panjang_note', 'ritme', 'kunci']\n",
    "\n",
    "# Membuat tokenizer untuk kolom 'bar'\n",
    "tokenizer = create_tokenizer(df)\n",
    "encodedDf = pd.DataFrame({})\n",
    "\n",
    "# Membuat peta balik dari token_id ke notasi ABC\n",
    "reverse_tokenizer = {v: k for k, v in tokenizer.items()}\n",
    "\n",
    "# Memproses kolom 'bar' dengan tokenizer\n",
    "encodedDf['bar_encoded'] = df['bar'].apply(lambda x: process_abc(x, tokenizer)[0] if pd.notnull(x) else [0])\n",
    "encodedDf['bar_length'] = df['bar'].apply(lambda x: process_abc(x, tokenizer)[1] if pd.notnull(x) else 0)\n",
    "\n",
    "# Encode kolom lainnya\n",
    "for col in pickedCol:\n",
    "    if col != 'bar':\n",
    "        encodedDf[col] = encoder.fit_transform(df[col])\n",
    "\n",
    "# Simpan DataFrame yang sudah diproses\n",
    "final_df = encodedDf.copy()\n",
    "\n",
    "# Konversi list of integers menjadi string agar kompatibel dengan GPT-2\n",
    "final_df['bar_encoded_str'] = final_df['bar_encoded'].apply(lambda x: ' '.join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Pelajaran\\Semester 7\\Riset\\Projek Coba-Coba\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bar_encoded</th>\n",
       "      <th>bar_length</th>\n",
       "      <th>pitch_pattern</th>\n",
       "      <th>birama</th>\n",
       "      <th>panjang_note</th>\n",
       "      <th>ritme</th>\n",
       "      <th>kunci</th>\n",
       "      <th>bar_encoded_str</th>\n",
       "      <th>input_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>33</td>\n",
       "      <td>2 3 4 5 6 7 8 9 10 11 12 13</td>\n",
       "      <td>Pitch pattern: 1, Birama: 2, Kunci: 33. Sequen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[14, 15, 6, 7, 16, 17, 10, 8, 13]</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>33</td>\n",
       "      <td>14 15 6 7 16 17 10 8 13</td>\n",
       "      <td>Pitch pattern: 1, Birama: 2, Kunci: 33. Sequen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[7, 18, 10, 19, 9, 3, 4, 8, 13, 6, 2, 17, 4, 2...</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>33</td>\n",
       "      <td>7 18 10 19 9 3 4 8 13 6 2 17 4 20 17</td>\n",
       "      <td>Pitch pattern: 1, Birama: 2, Kunci: 33. Sequen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[21, 15, 6, 7, 22, 17, 11, 20, 23, 21, 13]</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>33</td>\n",
       "      <td>21 15 6 7 22 17 11 20 23 21 13</td>\n",
       "      <td>Pitch pattern: 1, Birama: 2, Kunci: 33. Sequen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>33</td>\n",
       "      <td>2 3 4 5 6 7 8 9 10 11 12 13</td>\n",
       "      <td>Pitch pattern: 1, Birama: 2, Kunci: 33. Sequen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         bar_encoded  bar_length  \\\n",
       "0           [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]          12   \n",
       "1                  [14, 15, 6, 7, 16, 17, 10, 8, 13]           9   \n",
       "2  [7, 18, 10, 19, 9, 3, 4, 8, 13, 6, 2, 17, 4, 2...          15   \n",
       "3         [21, 15, 6, 7, 22, 17, 11, 20, 23, 21, 13]          11   \n",
       "4           [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]          12   \n",
       "\n",
       "   pitch_pattern  birama  panjang_note  ritme  kunci  \\\n",
       "0              1       2             0     31     33   \n",
       "1              1       2             0     31     33   \n",
       "2              1       2             0     31     33   \n",
       "3              1       2             0     31     33   \n",
       "4              1       2             0     31     33   \n",
       "\n",
       "                        bar_encoded_str  \\\n",
       "0           2 3 4 5 6 7 8 9 10 11 12 13   \n",
       "1               14 15 6 7 16 17 10 8 13   \n",
       "2  7 18 10 19 9 3 4 8 13 6 2 17 4 20 17   \n",
       "3        21 15 6 7 22 17 11 20 23 21 13   \n",
       "4           2 3 4 5 6 7 8 9 10 11 12 13   \n",
       "\n",
       "                                          input_text  \n",
       "0  Pitch pattern: 1, Birama: 2, Kunci: 33. Sequen...  \n",
       "1  Pitch pattern: 1, Birama: 2, Kunci: 33. Sequen...  \n",
       "2  Pitch pattern: 1, Birama: 2, Kunci: 33. Sequen...  \n",
       "3  Pitch pattern: 1, Birama: 2, Kunci: 33. Sequen...  \n",
       "4  Pitch pattern: 1, Birama: 2, Kunci: 33. Sequen...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Membuat direktori untuk menyimpan checkpoint jika belum ada\n",
    "checkpoint_dir = './gpt2_checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Tokenizer dan Model GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]', 'unk_token': '[UNK]'})\n",
    "\n",
    "# Menginisialisasi ulang model agar mengenali token tambahan\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Fungsi untuk menambahkan konteks ke teks input\n",
    "def add_context(row):\n",
    "    context = f\"Pitch pattern: {row['pitch_pattern']}, Birama: {row['birama']}, Kunci: {row['kunci']}.\"\n",
    "    return f\"{context} Sequence: {row['bar_encoded_str']}\"\n",
    "\n",
    "# Menerapkan fungsi untuk membuat kolom teks input lengkap\n",
    "final_df['input_text'] = final_df.apply(add_context, axis=1)\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pitch pattern: 1, Birama: 2, Kunci: 33. Sequence: 2 3 4 5 6 7 8 9 10 11 12 13'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.input_text.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 6, 8, 4, 9, 5, 7, 0, 1, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.birama.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33, 13, 29,  4, 12, 11,  0, 25,  5, 19,  3, 26, 18,  7,  8, 34, 30,\n",
       "        1,  2, 31, 21, 23, 15, 16,  9, 27, 17, 24, 10, 32, 14, 28, 35, 20,\n",
       "       36, 22,  6])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.kunci.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.pitch_pattern.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize the input sequences individually (tanpa padding)\n",
    "input_ids = [tokenizer(text, return_tensors='pt')['input_ids'].squeeze(0) for text in final_df['input_text'].tolist()]\n",
    "attention_masks = [torch.ones_like(ids) for ids in input_ids]  # Semua tokens dianggap penting, jadi masking 1\n",
    "\n",
    "# Create a simple custom dataset without padding\n",
    "class MusicDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids, attention_masks):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attention_masks[idx], self.input_ids[idx]\n",
    "\n",
    "dataset = MusicDataset(input_ids, attention_masks)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)  # batch_size=1 untuk menghindari padding\n",
    "print(f\"Total batches in dataloader: {len(dataloader)}\")\n",
    "\n",
    "# Training configuration\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "epochs = 1\n",
    "total_steps = len(dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Cek apakah CUDA tersedia\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "scaler = GradScaler()\n",
    "# Training loop\n",
    "model.train()\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_iterator = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False, dynamic_ncols=True)\n",
    "\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        input_ids_batch, attention_mask_batch, labels_batch = [x.to(device) for x in batch]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(input_ids=input_ids_batch, attention_mask=attention_mask_batch, labels=labels_batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Update tqdm manually\n",
    "        epoch_iterator.update(1)\n",
    "        epoch_iterator.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(dataloader):.4f}')\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "    }, os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pth'))\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine_tuned_gpt2')\n",
    "tokenizer.save_pretrained('./fine_tuned_gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LogitsProcessor\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "class CustomLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, valid_tokens):\n",
    "        self.valid_tokens = valid_tokens\n",
    "    \n",
    "    def __call__(self, input_ids, scores):\n",
    "        mask = torch.ones(scores.shape[-1], dtype=torch.bool, device=scores.device)\n",
    "        mask[self.valid_tokens] = False\n",
    "        scores[..., mask] = -float(\"inf\")\n",
    "        return scores\n",
    "\n",
    "# List of valid token IDs that correspond to your ABC notation tokens\n",
    "# Pastikan hanya angka valid yang disertakan\n",
    "# List of valid token IDs that correspond to your ABC notation tokens\n",
    "valid_token_ids = []\n",
    "for token, token_id in tokenizer.get_vocab().items():\n",
    "    try:\n",
    "        if token.isdigit() and 0 <= int(token) <= 100:\n",
    "            valid_token_ids.append(token_id)\n",
    "    except ValueError:\n",
    "        # Skip tokens that can't be converted to integer\n",
    "        continue\n",
    "\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('./fine_tuned_gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('./fine_tuned_gpt2')\n",
    "\n",
    "# Pastikan model dan tokenizer menggunakan perangkat yang tepat (CPU/GPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# List of valid token IDs that correspond to your ABC notation tokens\n",
    "valid_token_ids = [token_id for token, token_id in tokenizer.get_vocab().items() if token.isdigit()]\n",
    "\n",
    "# Create the logits processor\n",
    "logits_processor = CustomLogitsProcessor(valid_token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs size: 1024\n",
      "Sequence input terlalu panjang, tidak ada token baru yang dihasilkan.\n",
      "Decoded Music Sequence (ABC Notation):\n",
      "G>-A B-<cA^GAB c2(dc)(G>A) (BG/E/)G2G2 G2zTd>A (c/A/F/A/)b2(.B2.B2)b2\"D\"d/4^c/4d/4e/4 d/4=c/4B/4A/4]\\\n",
      "\"Gm\"G/4^F/4G/4A/4 \"D\"B/4A/4B/4c/4B>-G A>-B(~F>G) (AB/c/)](g^fga) g2d2(bgfd) cA.F2(.B>.B) (A{BA}G)BABc d2(d=e)B>B (A{BA}G)F>G A(B/A/)f4 f2z2\"Gm\"c/4B/4A/4B/4 GB2(AG) (BAG^F)\\\n",
      "\"Gm\"G/4^F/4G/4A/4 B/2G/2B2(AB) c2(Bc)f4 d2-f2G2G2 G3Gd>-=e f-<dF2D2 F2GAd>-=e f-<d\"Gm\"G/2G/2 G\\G3\"D\"D/2^F/2 A/2c/2c2d=e f2c2f4 (g2dB)(dcBA) (cBAG)BABc d2(cB)(~F>G) (AB/c/)G4 G2G2G2 G2z2d2g2 gfed(cB)(BA) (GB)(AG)B2d2 dcBAc>-A (F3/2G/4A/4)AFAB c2(dc)\"D\"^F/2A/2 A/4G/4F/2% Nottingham Music Database\n",
      "\"Gm\"d/4^c/4d/4e/4 \"D\"d/4=c/4B/4A/4\"Gm\"G/2B/2 B/4A/4G/2(d2{ed}c2) B2B2(g2a2 b2).g2BABc dedcB2AB cBAG(bagf) (gd)d>cD(f=efg) f2(dc)(GABc) (d2{ed}c>A)\">\"f4 e2(fe)(d2g2) g2d2F4 (D2=E^F)G3 GG>-A B-<cd>-B G>>D({d}c3-B) G2-E2cBAG BAGF(d2g2) g2d2(A2G2 {AG}F2).D2A-<d (3(cAG)G2GA B2AGB2d2 (gfdc)c>-A (F3/2G/4A/4)fgfd cdcBTd>A (c/A/F/A/)AF2F F=EFG.G(G/A/ B/G/F/D/)B2d2 c2f2B2d2 (Bcd=e)(dedc) B2d2\"Gm\"G/2B/2 B/4A/4G/2\">\"d4 c2(dc)A-<d c>ABG2G G^FGABG2F DEFD\"Gm\"d/4^c/4d/4e/4 \"D\"d/4=c/4B/4A/4GABc d2(cB)(GABc) d2d2fdcA FAcAA2)d2 (cBAc)G2A2 (B2{cB}AG)c2)(f2 B2)(f2(^FGF=E) D2 (dc)d>c BGGG3-A (Bcd=e).G(G A/G/F/D/)G/4^F/4G/4A/4 B/2G/2(ABcA) F2(GA)(~F>G) (AB/c/)GABc d2(d=e)AFAc BGAF(fgfe d2)(f2(e2d2) c2(fe)G4 G2G3-A (Bcd=e)d>e dc(cdcB) (ABAG)g.-f =e-<gB2c2 (dcAB)b2(ab c'bag)d2 \\F3 F(g3^f) (g2a)G3 G\"D\"^F/2A/2 D/2=E/4F/4(g^fga) g2d2(bagf) (edcB)(GABc) (d2{ed}c>A)BABG AG^FA.d/(b/a/^f/) g2dA c(A/G/)B2(AG) (BAG^F)\n"
     ]
    }
   ],
   "source": [
    "# Fungsi untuk memfilter data berdasarkan pitch_pattern, birama, dan kunci\n",
    "def filter_data(df, pitch_pattern, birama, kunci):\n",
    "    return df[(df['pitch_pattern'] == pitch_pattern) & \n",
    "              (df['birama'] == birama) & \n",
    "              (df['kunci'] == kunci)]\n",
    "\n",
    "# Fungsi untuk menggabungkan bar dari data yang difilter\n",
    "def combine_bars(filtered_df):\n",
    "    all_bars = filtered_df['bar_encoded'].tolist()\n",
    "    random.shuffle(all_bars)\n",
    "    \n",
    "    combined_bar = []\n",
    "    for bar in all_bars:\n",
    "        combined_bar.extend(bar)\n",
    "    \n",
    "    return combined_bar\n",
    "\n",
    "# Fungsi untuk menambahkan padding jika diperlukan\n",
    "def pad_sequence(sequence, max_length, pad_value=tokenizer.pad_token_id):\n",
    "    if len(sequence) < max_length:\n",
    "        sequence.extend([pad_value] * (max_length - len(sequence)))\n",
    "    return sequence\n",
    "\n",
    "\n",
    "# Fungsi untuk menghasilkan musik dengan menggunakan logits processor\n",
    "def generate_music_with_processor(model, tokenizer, pitch_pattern, birama, kunci, filtered_df, logits_processor, max_length=1024):\n",
    "    context = f\"Pitch pattern: {pitch_pattern}, Birama: {birama}, Kunci: {kunci}. Sequence: \"\n",
    "    combined_bar = combine_bars(filtered_df)\n",
    "    \n",
    "    min_sequence_length = 10  # Contoh nilai minimum\n",
    "\n",
    "    if len(combined_bar) < min_sequence_length:\n",
    "        raise ValueError(f\"Sequence terlalu pendek setelah filterisasi, panjang minimal adalah {min_sequence_length} tokens.\")\n",
    "\n",
    "    combined_bar_str = ' '.join(map(str, combined_bar))\n",
    "    \n",
    "    # Pastikan panjang sequence tidak melebihi max_length\n",
    "    combined_bar_tokens = combined_bar_str.split()\n",
    "    if len(combined_bar_tokens) > max_length:\n",
    "        combined_bar_tokens = combined_bar_tokens[:max_length]\n",
    "    \n",
    "    input_text = context + ' '.join(combined_bar_tokens)\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', padding=True, truncation=True, max_length=1024)\n",
    "\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    print(f\"Input IDs size: {inputs['input_ids'].size(1)}\")\n",
    "\n",
    "    # Menghitung max_new_tokens dengan memastikan tidak melebihi 1024\n",
    "    remaining_tokens = 1024 - inputs['input_ids'].size(1)\n",
    "    max_new_tokens = min(remaining_tokens, 50)\n",
    "    # Pastikan max_new_tokens lebih dari 0\n",
    "    if max_new_tokens > 0:\n",
    "        outputs = model.generate(\n",
    "            inputs['input_ids'], \n",
    "            max_new_tokens=max_new_tokens,  # Ini memastikan max_new_tokens valid\n",
    "            num_return_sequences=1, \n",
    "            pad_token_id=tokenizer.pad_token_id, \n",
    "            bad_words_ids=[[tokenizer.pad_token_id]],\n",
    "            repetition_penalty=2.5,\n",
    "            no_repeat_ngram_size=3,\n",
    "            do_sample=False,  # Nonaktifkan sampling\n",
    "            logits_processor=[logits_processor] if logits_processor else None\n",
    "        )\n",
    "\n",
    "        generated_sequence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return generated_sequence\n",
    "    else:\n",
    "        # Mengembalikan sequence input jika tidak ada ruang untuk token baru\n",
    "        print(\"Sequence input terlalu panjang, tidak ada token baru yang dihasilkan.\")\n",
    "        return combined_bar_str  # Mengembalikan sequence asli\n",
    "\n",
    "\n",
    "# Fungsi untuk membersihkan hasil yang dihasilkan\n",
    "def clean_generated_sequence(sequence):\n",
    "    # Filter out non-numeric tokens or invalid sequences\n",
    "    filtered_sequence = [token for token in sequence.split() if token.isdigit() and 0 <= int(token) <= 100]\n",
    "    return ' '.join(filtered_sequence)\n",
    "\n",
    "# Fungsi untuk mengembalikan angka-angka menjadi notasi ABC\n",
    "def decode_to_abc(sequence, reverse_tokenizer):\n",
    "    tokens = sequence.split()  # Memisahkan urutan angka ke dalam daftar\n",
    "    abc_notation = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        decoded_token = reverse_tokenizer[int(token)]\n",
    "        if abc_notation and (decoded_token.isdigit() or decoded_token in ['/', '^', '=', '-', '<', '>']):\n",
    "            # Jika token adalah angka atau simbol yang berkaitan dengan not sebelumnya, gabungkan tanpa spasi\n",
    "            abc_notation[-1] += decoded_token\n",
    "        else:\n",
    "            # Jika token adalah not atau karakter baru, tambahkan sebagai elemen baru\n",
    "            abc_notation.append(decoded_token)\n",
    "\n",
    "    return ''.join(abc_notation)  # Gabungkan semua tanpa spasi tambahan\n",
    "\n",
    "\n",
    "# Inputan dari pengguna\n",
    "pitch_pattern_input = 1\n",
    "birama_input = 2\n",
    "kunci_input = 33\n",
    "\n",
    "# Filter data sesuai dengan input\n",
    "filtered_df = filter_data(final_df, pitch_pattern_input, birama_input, kunci_input)\n",
    "\n",
    "# Setelah mendapatkan hasil dari model dan membersihkannya\n",
    "if not filtered_df.empty:\n",
    "    generated_music = generate_music_with_processor(model, tokenizer, pitch_pattern_input, birama_input, kunci_input, filtered_df, logits_processor)\n",
    "    \n",
    "    # Membersihkan hasil yang dihasilkan\n",
    "    cleaned_music = clean_generated_sequence(generated_music)\n",
    "    \n",
    "    # Mengonversi urutan angka ke notasi ABC\n",
    "    decoded_music = decode_to_abc(cleaned_music, reverse_tokenizer)  # Gunakan reverse_tokenizer di sini\n",
    "    \n",
    "    print(\"Decoded Music Sequence (ABC Notation):\")\n",
    "    print(decoded_music)\n",
    "else:\n",
    "    print(\"Tidak ditemukan data yang sesuai dengan input yang diberikan.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
